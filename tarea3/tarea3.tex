% vim: set fileencoding=utf-8 encoding=utf-8 tw=100:
\documentclass[spanish]{article}
\usepackage{babel}
\usepackage[utf8]{inputenc}
\usepackage{fullpage}
\usepackage{url}
\usepackage{palatino}
\usepackage{mathpazo}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}

\newcommand{\pregunta}{\textit}
\newcommand{\given}{\vert}
\newcommand{\card}[1]{\lvert#1\rvert}
\newcommand{\abs}[1]{\lvert#1\rvert}
\newcommand{\norm}[1]{\lVert#1\rVert}
\newcommand{\R}{\mathbb{R}}

\title{Reconocimiento de Patrones---Tarea 3}
\author{Roberto Bonvallet \\ \url {<rbonvall@gmail.com>}}
\date{Noviembre de 2008}

\begin{document}
\maketitle

\section*{Pregunta 1}
\pregunta{
    Estudiar los conceptos de métrica, distancia y similaridad.
    ¿Son estos conceptos equivalentes?
    Justifique usando algunos ejemplos.
}

Una distancia es una medida de cuán diferentes son dos elementos.  Es una
función $d:X\times X\to\R_0^+$, donde $X$ es el espacio de características en
que están representados los elementos.  Una distancia satisface\footnotemark:
\begin{align*}
    d(x, y) &\ge 0, \\ d(x, x) &= 0, \\ d(x, y) &= d(y, x).
\end{align*}
\footnotetext{\emph{Medidas de Asociación},
    \url{http://www.ugr.es/~gallardo/pdf/cluster-2.pdf}.}
Una métrica es una distancia que además satisface los axiomas métricos:
\begin{align*}
    d(x, y) &= 0\implies x = y, \\
    d(x, z) &\le d(x, y) + d(y, z).
\end{align*}







\section*{Pregunta 2}
\pregunta{Estudiar las diferentes linkage métricas para dos clústers:}
\begin{itemize}
    \item \pregunta{single link:} la distancia entre el par de elementos más cercano de los
        clústers:
        \[d(C_1, C_2) = \min_{C_1\times C_2}\bigl\{d(x_1, x_2)\bigr\}.\]
    \item \pregunta{complete link:} la distancia entre el par de elementos más lejanos de los
        clústers:
        \[d(C_1, C_2) = \max_{C_1\times C_2}\bigl\{d(x_1, x_2)\bigr\}.\]
    \item \pregunta{centroid link:} la distancia entre los centroides de los clústers:
        \[d(C_1, C_2) = d\left(\frac{1}{\card{C_1}}\sum_{C_1} x_1, \frac{1}{\card{C_2}}\sum_{C_2} x_2\right).\]
    \item \pregunta{average link:} el promedio de las distancias entre todos los pares de elementos
        de los clústers:
        \[d(C_1, C_2) = \frac{1}{\card{C_1}\card{C_2}}\sum_{C_1\times C_2} d(x_1, x_2).\]
\end{itemize}

\section*{Pregunta 3}
\pregunta{Proponer medidas de similaridad para las siguientes representaciones:}
    \begin{itemize}
        \item \pregunta{datos cuantitativos
            (señales ECG, matrices, funciones, tensores, imágenes digitales, etc.),}

            Para señales continuas, distancias típicas son las métricas de Minkowski y
            del supremo:
            \begin{align}
                d_{\text{(Mink)}}(x, y)^p &= \int\bigl\lvert x(t) - y(t)\bigr\rvert^{p}\,dt; &
                d_{\text{(sup)}}(x, y) &= \sup_t \Bigl\{\bigl\lvert x(t) - y(t)\bigr\rvert\Bigr\}.
            \end{align}
            Estas métricas tienen sus equivalentes en el caso discreto:
            \begin{align}
                d_{\text{(Mink)}}(x, y)^p &= \sum_k \abs{x_k - y_k}^p; &
                d_{\text{(sup)}}(x, y) &= \max_k\bigl\{\abs{x_k - y_k}\bigr\}.
            \end{align}
            Estas distancias se pueden adaptar para considerar sólo características de interéde
            interés de la
            señal, como bandas específicas en el espacio de frecuencias, o escalas en un análisis
            multiresolución:
            \begin{align}
                d_{\text{(F)}}(x, y)^p   &= 
                    \sum_{\text{frecs. $f$ de interés}} \abs{X_f - Y_f}^p \\
                d_{\text{(MRA)}}(x, y)^p &= 
                    \sum_{\text{escalas $s$ de interés}}
                    \bigl\lvert\langle f - g, \psi_s \rangle\,\psi_s \bigr\rvert^p
            \end{align}

        \item \pregunta{datos categóricos
            (documentos, libros, símbolos, jeroglíficos, etc.)}

            Documentos y libros se pueden representar como vectores de pesos, en que cada componente
            está asociado a una palabra $k_i$, y está relacionado con la frecuencia con que aparece en el
            texto:
            \begin{equation}
                \text{documento $d$} = (w_1, \ldots, w_t)
            \end{equation}
            Bajo esta representación, se puede utilizar la correlación estadística como medida de
            similaridad entre documentos:
            \begin{equation}
                \text{sim}(d_1, d_2) = \frac{d_1\cdot d_2}{\norm{d_1}\norm{d_2}}
            \end{equation}
            Para asignar los pesos, se puede utilizar el esquema tf-idf:  $w_i =
            \text{tf}_{i,d}\cdot\text{idf}_i$, donde tf es una medida de
            frecuencia de aparición de la palabra $k_i$ en $d$, e idf está
            asociado a en cuántos documentos aparece $k_i$.~\footnotemark
            \footnotetext{Ricardo Baeza, Berthier Ribeiro. \emph{Modern
            Information Retrieval}.}

            Para cadenas de símbolos, se puede utilizar la distancia de
            Levenshtein, que está dada por el mínimo número de operaciones
            necesarias para transformar una cadena en otra, donde las
            operaciones definidas son: la inserción, la eliminación o el
            reemplazo de un símbolo.  No hay una fórmula para esta distancia, y
            debe ser calculada algorítmicamente.

            La similaridad entre símbolos puede ser definida ``a mano'',
            asignando valores de similaridad a cada par de símbolos.  En el caso
            de símbolos que tienen características propias, se puede definir
            medidas de similaridad a partir de estas características.  Por
            ejemplo, los caracteres que se utilizan para escribir dialectos
            chinos (por ejemplo, el mandarín) son clasificados tradicionalmente
            por: el número de trazos que se utiliza para escribirlos, los
            radicales (componentes) que lo componen, y por su pronunciación.
            Una medida de similaridad entre dos caracteres puede combinar:
            la diferencia entre el número de trazos, la cantidad de radicales
            comunes y las semejanzas fonéticas (consonante, vocal y tono).

    \end{itemize}

\section*{Pregunta 4}
\pregunta{Proponer medidas de similaridad para mezclas de datos.}

Una opción para medir similiaridad entre mezclas de datos cuantitativos y categóricos es crear
categorías para los datos cuantitativos, p.ej. $x\in C_k\iff A_k\leq x\leq A_{k+1}$, y utilizar
medidas para datos categóricos.  El problema con este enfoque es que se pierde información
inherente a la naturaleza cuantitativa de los datos, como las relaciones de orden.

La métrica de Minkowski se puede adaptar para mezclas de datos reemplazando $\abs{x_k - y_k}$ por
una expresión que represente la distancia a lo largo de una dimensión, adaptada para datos
categóricos.  Por ejemplo, la métrica de diferencia de valor (VDM)~\footnotemark utiliza la
expresión para la $j$-ésima componente en problemas de clasificación:
\begin{equation}
    d_j(y, z) = \sum_{s}\bigl\lvert P(\omega_s\given x_j = y_j) -
                                    P(\omega_s\given x_j = z_j)\bigr\rvert,
\end{equation}
donde las probabilidades se pueden estimar a partir de las frecuencias en los datos de
entrenamiento.
\footnotetext{Lars Haendel. \textit{The \textsc{Pnc2} Cluster Algorithm}. Tesis doctoral, Universidad
de Dortmund, 2003.}

Otra medida es el coeficiente general de similaridad de Gower:
\begin{equation}
   s_{ij} = \frac{\sum_k w_{ijk} s_{ijk}}{\sum_k w_{ijk}},
\end{equation}
donde $s_{ijk}$ es la similaridad a lo largo de la $k$-ésima dimensión (definida
de acuerdo a la naturaleza de esa característica)
y $w_{ijk}$ mide la relevancia de la $k$-ésima componente para esta comparación.
Los $w_{ijk}$ son definidos por un usuario, y sirven además para manejar datos
faltantes ($w_{ijk} = 0$).

Un enfoque similar al de Gower es la introducción de una matriz de similaridad
combinada\footnotemark
\footnotetext{Charles Romesburg. \emph{Cluster Analysis for Researchers}.
Chapter~12: \emph{Resemblance Coefficients for Mixtures of Quantitative and
Qualitative Attributes}.}

%\footnotemark
%\footnotetext{Michael Berthold. \textit{Mixed fuzzy rule formation}.  International Journal of
%Approximate Reasoning 32~(2003) 67--84.
%
%
\end{document}
