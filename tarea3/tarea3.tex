% vim: set fileencoding=utf-8 encoding=utf-8 tw=100:
\documentclass[spanish]{article}
\usepackage{babel}
\usepackage[utf8]{inputenc}
\usepackage{fullpage}
\usepackage{url}
\usepackage{palatino}
\usepackage{mathpazo}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}

\newcommand{\pregunta}{\textit}
\newcommand{\given}{\vert}
\newcommand{\card}[1]{\lvert#1\rvert}
\newcommand{\abs}[1]{\lvert#1\rvert}
\newcommand{\norm}[1]{\lVert#1\rVert}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}

\title{Reconocimiento de Patrones---Tarea 3}
\author{Roberto Bonvallet \\ \url {<rbonvall@gmail.com>}}
\date{Noviembre de 2008}

\begin{document}
\maketitle

\section*{Pregunta 1}
\pregunta{
    Estudiar los conceptos de métrica, distancia y similaridad.
    ¿Son estos conceptos equivalentes?
}

Una distancia es una medida de cuán diferentes son dos elementos.  Es una
función $d:X\times X\to\R_0^+$, donde $X$ es el espacio de características en
que están representados los elementos.  Una distancia satisface\footnotemark:
\begin{align*}
    d(x, y) &\ge 0, \\ d(x, x) &= 0, \\ d(x, y) &= d(y, x).
\end{align*}
\footnotetext{\emph{Medidas de Asociación},
    \url{http://www.ugr.es/~gallardo/pdf/cluster-2.pdf}.}
Una métrica es una distancia que además satisface los axiomas métricos:
\begin{align*}
    d(x, y) &= 0\implies x = y, \\
    d(x, z) &\le d(x, y) + d(y, z).
\end{align*}

Una similaridad es una medida de cuán parecidos son dos elementos.  Una similaridad $s$ satisface:
\begin{align*}
    s(x, x) &= s_0, \\
    s(x, y) &\le s_0, \\
    s(x, y) = s(y, x).
\end{align*}

Distancia y similaridad son conceptos equivalentes, ya que se pueden expresar uno en función del
otro; por ejemplo: $s(x, y) = 1 - d(x, y)$.  El concepto de métrica es más restrictivo y por lo
tanto no es equivalente.


\section*{Pregunta 2}
\pregunta{Estudiar las diferentes linkage métricas para dos clústers:}
\begin{itemize}
    \item \pregunta{single link:} la distancia entre el par de elementos más cercano de los
        clústers:
        \[d(C_1, C_2) = \min_{C_1\times C_2}\bigl\{d(x_1, x_2)\bigr\}.\]
    \item \pregunta{complete link:} la distancia entre el par de elementos más lejanos de los
        clústers:
        \[d(C_1, C_2) = \max_{C_1\times C_2}\bigl\{d(x_1, x_2)\bigr\}.\]
    \item \pregunta{centroid link:} la distancia entre los centroides de los clústers:
        \[d(C_1, C_2) = d\left(\frac{1}{\card{C_1}}\sum_{C_1} x_1, \frac{1}{\card{C_2}}\sum_{C_2} x_2\right).\]
    \item \pregunta{average link:} el promedio de las distancias entre todos los pares de elementos
        de los clústers:
        \[d(C_1, C_2) = \frac{1}{\card{C_1}\card{C_2}}\sum_{C_1\times C_2} d(x_1, x_2).\]
\end{itemize}

\section*{Pregunta 3}
\pregunta{Proponer medidas de similaridad para las siguientes representaciones:}
    \begin{itemize}
        \item \pregunta{datos cuantitativos
            (señales ECG, matrices, funciones, tensores, imágenes digitales, etc.),}

            Los datos cuantitativos suelen ser representados como vectores en un espacio $\R^n$ o
            $\C^n$.  Para señales continuas, distancias típicas son las métricas de Minkowski y
            del supremo:
            \begin{align}
                d_{\text{(Mink)}}(x, y)^p &= \int\bigl\lvert x(t) - y(t)\bigr\rvert^{p}\,dt; &
                d_{\text{(sup)}}(x, y) &= \sup_t \Bigl\{\bigl\lvert x(t) - y(t)\bigr\rvert\Bigr\}.
            \end{align}
            Estas métricas tienen sus equivalentes en el caso discreto:
            \begin{align}
                d_{\text{(Mink)}}(x, y)^p &= \sum_k \abs{x_k - y_k}^p; &
                d_{\text{(sup)}}(x, y) &= \max_k\bigl\{\abs{x_k - y_k}\bigr\}.
            \end{align}
            Estas distancias se pueden adaptar para considerar sólo características de interéde
            interés de la
            señal, como bandas específicas en el espacio de frecuencias, o escalas en un análisis
            multiresolución:
            \begin{align}
                d_{\text{(F)}}(x, y)^p   &= 
                    \sum_{\text{frecs. $f$ de interés}} \abs{X_f - Y_f}^p, \\
                d_{\text{(MRA)}}(x, y)^p &= 
                    \sum_{\text{escalas $s$ de interés}}
                    \bigl\lvert\langle f - g, \psi_s \rangle\,\psi_s \bigr\rvert^p.
            \end{align}
            Si se conocen las covariancias entre las características, se puede utilizar la distancia
            de Mahalanobis:
            \begin{equation}
                d_{\text{(Mah)}(x, y)^2 = \sum_{i, j} \sigma_ij x_i y_j.
            \end{equation}

        \item \pregunta{datos categóricos
            (documentos, libros, símbolos, jeroglíficos, etc.)}

            Documentos y libros se pueden representar como vectores de pesos, en que cada componente
            está asociado a una palabra $k_i$, y está relacionado con la frecuencia con que aparece en el
            texto:
            \begin{equation}
                \text{documento $d$} = (w_1, \ldots, w_t)
            \end{equation}
            Bajo esta representación, se puede utilizar la correlación estadística como medida de
            similaridad entre documentos:
            \begin{equation}
                \text{sim}(d_1, d_2) = \frac{d_1\cdot d_2}{\norm{d_1}\norm{d_2}}
            \end{equation}
            Para asignar los pesos, se puede utilizar el esquema tf-idf:  $w_i =
            \text{tf}_{i,d}\cdot\text{idf}_i$, donde tf es 


            Para cadenas de símbolos, se puede utilizar la distancia de
            Levenshtein, que está dada por el mínimo número de operaciones
            necesarias para transformar una cadena en otra, donde las
            operaciones definidas son: la inserción, la eliminación o el
            reemplazo de un símbolo.  No hay una fórmula para esta distancia, y
            debe ser calculada algorítmicamente.


    \end{itemize}

\section*{Pregunta 4}
\pregunta{Proponer medidas de similaridad para mezclas de datos.}

Una opción para medir similiaridad entre mezclas de datos cuantitativos y categóricos es crear
categorías para los datos cuantitativos, p.ej. $x\in C_k\iff A_k\leq x\leq A_{k+1}$, y utilizar
medidas para datos categóricos.  El problema con este enfoque es que se pierde información
inherente a la naturaleza cuantitativa de los datos, como las relaciones de orden.

La métrica de Minkowski se puede adaptar para mezclas de datos reemplazando $\abs{x_k - y_k}$ por
una expresión que represente la distancia a lo largo de una dimensión, adaptada para datos
categóricos.  Por ejemplo, la métrica de diferencia de valor (VDM)~\footnotemark utiliza la
expresión para la $j$-ésima componente en problemas de clasificación:
\begin{equation}
    d_j(y, z) = \sum_{s}\bigl\lvert P(\omega_s\given x_j = y_j) -
                                    P(\omega_s\given x_j = z_j)\bigr\rvert,
\end{equation}
donde las probabilidades se pueden estimar a partir de las frecuencias en los datos de
entrenamiento.
\footnotetext{Lars Haendel. \textit{The \textsc{Pnc2} Cluster Algorithm}. Tesis doctoral, Universidad
de Dortmund, 2003.}

%\footnotetex{Michael Berthold. \textit{Mixed fuzzy rule formation}.  International Journal of
%Approximate Reasoning 32~(2003) 67--84.


\end{document}
